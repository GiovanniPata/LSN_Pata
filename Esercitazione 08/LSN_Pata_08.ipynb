{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986096d0",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> NUMERICAL EXERCISES 08 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329c310",
   "metadata": {},
   "source": [
    "###### QUANTUM VARIATIONAL MC\n",
    "\n",
    "- This JN aims to find a good approximation of the ground state $\\Psi_0$ for the Hamiltonian\n",
    "\n",
    "$$\n",
    "\\hat H = -\\frac{\\hbar^2}{2m}\\Delta + V(x)\n",
    "$$\n",
    "\n",
    "   where $V(x)=x^4-\\frac 5 2 x^2$ and $\\hbar=m=1$.\n",
    "\n",
    "\n",
    "- A good approximation is done by searching for the best trial function of the form\n",
    "\n",
    "$$\n",
    "\\Psi_T^{\\sigma,\\mu}(x) \\propto e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}+\n",
    "                               e^{-\\frac{(x+\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "   where $\\sigma,\\mu$ are (positive) tunable parameters.\n",
    "\n",
    "- Optimization in the space of these functions is achieved by exploiting the variational principle\n",
    "\n",
    "$$\n",
    "\\langle {\\hat H} \\rangle_T = \n",
    "\\frac{\\int dx \\Psi^*_T(x) {\\hat H} \\Psi_T(x)}\n",
    "{\\int dx |\\Psi_T(x)|^2} \\ge E_0 =\n",
    "\\frac{\\langle \\Psi_0| {\\hat H} | \\Psi_0 \\rangle}\n",
    "{\\langle \\Psi_0 | \\Psi_0 \\rangle}\n",
    "$$\n",
    "\n",
    "- The eigenfunctions I will find will be compared with those obtained from the diagonalization of the discretized Hamiltonian (discretization = continuous --> 1000 points in [-5.5]; laplacian --> discretized laplacian).\n",
    "\n",
    "\n",
    "- Taking advantage of the parity of the potential and using fundamental quantum mechanical arguments, one could find the first excited state $\\Psi_1$, which will be an odd function, using the variational principle on the space of functions (with node in the origin) of the form\n",
    "\n",
    "$$\n",
    "\\Psi_T^{\\sigma,\\mu}(x) \\propto e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}-\n",
    "                               e^{-\\frac{(x+\\mu)^2}{2\\sigma^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0856d1d",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 08.1 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39aa8f1",
   "metadata": {},
   "source": [
    "###### MC INTEGRAL\n",
    "\n",
    "First of all I need a c++ code that involves calculating the integral $\\langle {\\hat H} \\rangle_T$, fixed $\\sigma,\\mu$, using the Metropolis algorithm.\n",
    "\n",
    "Manipulating the exepression of the integral yields the following expression:\n",
    "\n",
    "$$\n",
    "\\langle {\\hat H} \\rangle_T = \n",
    "\\frac{\\int dx \\Psi^*_T(x) {\\hat H} \\Psi_T(x)}\n",
    "{\\int dx |\\Psi_T(x)|^2} = \n",
    "\\int dx \\frac{|\\Psi_T(x)|^2}{\\int dx |\\Psi_T(x)|^2} \\frac{{\\hat H} \\Psi_T(x)}{\\Psi_T(x)}  \\dot= \n",
    "{\\int dx \\rho(x) f(x)}\n",
    "$$\n",
    "\n",
    "The integral is written as the integral of a probability distribution $\\rho$ times a function $f$, so once I can sample $\\rho$, I can solve the integral by the importance sampling method.\n",
    "\n",
    "To perform the sampling I exploit the Metropolis algorithm . Given a point $x_n$ extracted from the distribution, I can extract the next $x_{n+1}$ by proposing a point $y$ generated with uniform probability in $[x_n-walk, x_n+walk]$. $walk$ is chosen so as to satisfy the 50/50 rule. The acceptance probability of $y$ will take a simple form because we simplify both the transition probabilities (due to their symmetry) and rho normalizations (which appear in numerator and denominator). If the move is accepted, then $x_{n+1}=y$, else $x_{n+1}=x_n$. \n",
    "\n",
    "I don't have to wait for equilibration because I choose zero as the starting point, which is a very likely point and therefore close to equilibrium.\n",
    "\n",
    "The graph below represent the progressive value of $\\langle {\\hat H} \\rangle_T$ (with the a posteriori values $\\mu =0.8$, $\\sigma=0.6$) with error bars, as a function of the number of blocks $N$ ($M=1,000,000$, $N=100$, $L=10,000$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5272fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "M=1000000              # Total number of throws\n",
    "N=100                 # Number of blocks\n",
    "L=int(M/N)            # Number of throws in each block, please use for M a multiple of N\n",
    "\n",
    "x = np.arange(N)      # [0,1,2,...,N-1]\n",
    "\n",
    "sum_prog, err_prog = np.loadtxt(\"data/integrale.txt\", usecols =(1,2), unpack = 'true')\n",
    "    \n",
    "x*=L # Number of throws = block * (Number of throws in each block)\n",
    "\n",
    "plt.errorbar(x,sum_prog,yerr=err_prog)\n",
    "plt.title('Integral with mu=0.8, sigma=0.6')\n",
    "plt.xlabel('#throws')\n",
    "plt.ylabel('value of integral')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496960b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Exercise 08.2 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435cc5c",
   "metadata": {},
   "source": [
    "###### SIMULATED ANNEALING\n",
    "\n",
    "To perfrom the optimization of $\\mu$ and $\\sigma$ parameters, I have used the Simulated Annealing (SA) algorithm.\n",
    "The SA algorithm is based on the analogy with the formation of crystal lattices: a fast-frozen molten metal will become an amorphous crystal (local minimization), while a slow cooling, ideally passing through infinite states of thermal equilibrium, will prevent the occurrence of local defects (global minimization).\n",
    "The idea is to do an embedding of the present optimization problem in the Gibbs ensemble, so that it can be solved with the Metropolis algorithm, using the identifications: ($\\mu$, $\\sigma$)=state, $\\langle {\\hat H} \\rangle_{\\mu, \\sigma} \\dot=F(\\mu,\\sigma)$=energy, minimizing state=lower-energy state.\n",
    "\n",
    "The implementation of SA is as follows:\n",
    "1) set random initial values of parameters in the range of interest (e.g.$\\mu = \\sigma=1$);\\\n",
    "2) set a (high) starting temperature, say $T=10$;\\\n",
    "3) extract new states with the Metropolis algorithm, choosing a transition probability ($\\mu_{new}$ extracted uniformly in $[\\mu-0.05, \\mu+0.05]$ and $\\sigma_{new}=r\\sigma$ with $r$ extracted uniformly in $[0.95, 1.05]$);\\\n",
    "4) once the Metropolis converges to the equilibrium distribution, lower the temperature and return to step 1).\n",
    "\n",
    "\n",
    "The algorithm leaves at least three sensitive questions open:\n",
    "- How much to decrease the temperature at each SA step?\n",
    "- When is thermal equilibrium reached at a fixed temperature?\n",
    "- When does the algorithm terminate?\n",
    "\n",
    "For the first question, I chose a gemoetric trend of temperature: at each step it decreased by multiplying by $R=0.75$.\\\n",
    "The second question is tricky: I should check that equilibration is achieved at each temperature, for example, by looking by eye when the graph of $F(\\mu,\\sigma)$ stabilizes itself. This check will be done a posteriori on the final graphs. The choice is to generate 100 states for each temperature, trusting that no equilibration is needed, since:\\\n",
    "A-the initial state is immediately in equilibrium (it is at almost infinite temperature);\\\n",
    "B-each state starts near equilibrium since it has similar temperature to the previous one.\\\n",
    "For the third question, I choose to stop when $T<0.0001$ (about 40 steps): at this value, in fact, the probability of accepting moves becomes practically zero: frozen system).\n",
    "\n",
    "The final values of $\\sigma$ and $\\mu$ will be those that minimize the mean value of the Hamiltonian. The uncertainties with which I will know these two values will not be too much less than the individual steps in which I varied them ($0.05$ for mu, $0.05 \\sigma = 0.03$ for sigma).\n",
    "\n",
    "__________________________________________________________________________________________________________________________\n",
    "\n",
    "I made 4 graphs:\n",
    "\n",
    "1) Progressive estimates, with error bars, of $F(\\mu, \\sigma)$ as a function of the steps in the SA (so one value per temperature);\\\n",
    "2) Path plots of the $\\mu$ (x-axis) and $\\sigma$ (y-axis) minimizing parameters for each step of the SA;\\\n",
    "(these two plots are used to summarize the SA and visualize that the choice to stop at T=0.0001 is sensible because mu and sigma do not move more)\\\n",
    "3) Progressive estimation with error of $\\langle {\\hat H} \\rangle_{\\mu, \\sigma}$  as a function of the number of blocks, for the best $\\mu$ and $\\sigma$ parameters (exercise code 08.1 is used);\\\n",
    "4) Plot of $|\\Psi(x)|^2$ for the ground state comparing: theoretical curve with $\\mu$ and $\\sigma$ minimizing, curve given by solution of discretized problem, curve given by the sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d137eef",
   "metadata": {},
   "source": [
    "###### BEST VALUES\n",
    "\n",
    "$\\mu = 0.809$\\\n",
    "$\\sigma = 0.619$\\\n",
    "$F(\\mu, \\sigma) = -0.445$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308687c4",
   "metadata": {},
   "source": [
    "###### GRAPHS\n",
    "\n",
    "1) Progressive estimates, with error bars, of $F(\\mu, \\sigma)$ as a function of the steps in the SA (so one value per temperature)\n",
    "\n",
    "We can't see the error bars because they are too small, but we see that the value stabilizes in the latter stages of the SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2203fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n=41                      # Total number of SA steps (temperatures)\n",
    "\n",
    "x = np.arange(n)      # [0,1,2,...,N-1]\n",
    "y, err = np.loadtxt(\"data/energiaSA.txt\",usecols=(0,1), unpack = 'true')\n",
    " \n",
    "plt.errorbar(x,y, yerr=err)\n",
    "plt.title('Stabilization of mean value of F(mu,sigma)')\n",
    "plt.xlabel('SA steps')\n",
    "plt.ylabel('<H>')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c1572",
   "metadata": {},
   "source": [
    "2) Path plots of the $\\mu$ (x-axis) and $\\sigma$ (y-axis) minimizing parameters for each step of the SA + stabilization of $\\mu$ and $\\sigma$.\n",
    "\n",
    "Similar to the previous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n=41                      # Total number of SA steps (temperatures)\n",
    "\n",
    "a=np.arange(n)\n",
    "x,y = np.loadtxt(\"data/parametriSA.txt\",usecols=(2,3), unpack = 'true')\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title('Path of mu and sigma in SA')\n",
    "plt.xlabel('mu')\n",
    "plt.ylabel('sigma')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(a,x)\n",
    "plt.title('Stabilization of mu')\n",
    "plt.xlabel('SA steps')\n",
    "plt.ylabel('mu')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(a,y)\n",
    "plt.title('Stabilization of sigma')\n",
    "plt.xlabel('SA steps')\n",
    "plt.ylabel('sigma')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341864ff",
   "metadata": {},
   "source": [
    "3) Progressive estimation with error of $\\langle {\\hat H} \\rangle_{\\mu, \\sigma}$  as a function of the number of blocks, for the minimizing $\\mu$ and $\\sigma$ parameters.\n",
    "\n",
    "$M=1,000,000$, $N=100$, $L=10,000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c28af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "M=1000000              # Total number of throws\n",
    "N=100                 # Number of blocks\n",
    "L=int(M/N)            # Number of throws in each block, please use for M a multiple of N\n",
    "\n",
    "x = np.arange(N)      # [0,1,2,...,N-1]\n",
    "\n",
    "sum_prog, err_prog = np.loadtxt(\"data/integrale.txt\", usecols =(1,2), unpack = 'true')\n",
    "    \n",
    "x*=L # Number of throws = block * (Number of throws in each block)\n",
    "\n",
    "plt.errorbar(x,sum_prog,yerr=err_prog)\n",
    "plt.title('Integral with best values of mu and sigma')\n",
    "plt.xlabel('#throws')\n",
    "plt.ylabel(' Integral')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600297dc",
   "metadata": {},
   "source": [
    "4) Plot of $|\\Psi(x)|^2$ for the ground state comparing: theoretical curve with $\\mu$ and $\\sigma$ minimizing, curve given by solution of discretized problem, curve given by the sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c182ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#1) Discrtetized problem: see exercise guide\n",
    "\n",
    "def Vpot(x):\n",
    "    return (x**2 - 2.5)*x**2\n",
    "    #return 0.5*x**2\n",
    "\n",
    "hbar = 1\n",
    "m = 1\n",
    "a = 10\n",
    "N = 1000 # number of iterations\n",
    "\n",
    "# Step sizes\n",
    "x = np.linspace(-a/2, a/2, N)\n",
    "dx = x[1] - x[0] # the step size\n",
    "V = Vpot(x)\n",
    "\n",
    "# The central differences method: f\" = (f_1 - 2*f_0 + f_-1)/dx^2\n",
    "\n",
    "CDiff = np.diag(np.ones(N-1),-1)-2*np.diag(np.ones(N),0)+np.diag(np.ones(N-1),1)\n",
    "# np.diag(np.array,k) construct a \"diagonal\" matrix using the np.array\n",
    "# The default is k=0. Use k>0 for diagonals above the main diagonal, \n",
    "# and k<0 for diagonals below the main diagonal\n",
    "\n",
    "# Hamiltonian matrix\n",
    "H = (-(hbar**2)*CDiff)/(2*m*dx**2) + np.diag(V)\n",
    "\n",
    "# Compute eigenvectors and their eigenvalues\n",
    "E,psi = np.linalg.eigh(H)\n",
    "\n",
    "# Take the transpose & normalize\n",
    "psi = np.transpose(psi)\n",
    "psi = psi/np.sqrt(dx)\n",
    "\n",
    "print(\"Ground state energy: \", E[0])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "scale = 0.3\n",
    "plt.plot(x,(psi[0])**2, label='discretized problem')\n",
    "\n",
    "\n",
    "#2) Sampling curve\n",
    "\n",
    "x,y=np.loadtxt(\"data/histo_best.txt\", usecols =(0,1), unpack = 'true')\n",
    "plt.plot(x,y, label='sampling curve')\n",
    "\n",
    "#3) Curve with best values (not normalized)\n",
    "\n",
    "def f(x,mu,sigma):\n",
    "    a1=np.exp(-(x+mu)**2/(2*sigma**2))\n",
    "    a2=np.exp(-(x-mu)**2/(2*sigma**2))\n",
    "    return (a1+a2)**2\n",
    "mu=0.809\n",
    "sigma=0.619\n",
    "\n",
    "x = np.linspace(-a/2, a/2, N)\n",
    "y = 0.5*f(x,mu,sigma)\n",
    "plt.plot(x,y, label='un-normalized theoretical curve')\n",
    "\n",
    "\n",
    "#PLOT\n",
    "plt.title(\"Best wavefunction\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.grid(True)\n",
    "plt.xlim((-3,3))\n",
    "plt.ylim((-0.6,0.6))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32649f",
   "metadata": {},
   "source": [
    "##### CONCLUSIONS\n",
    "\n",
    "The final values found are:\n",
    "$\\mu = 0.809$, \n",
    "$\\sigma = 0.619$, \n",
    "$F(\\mu, \\sigma) = -0.445$.\n",
    "\n",
    "1) Simulated Annealing worked well: I started from a random state and by cooling the system more and more I arrived at an equilibrium configuration that minimizes energy.\n",
    "\n",
    "2) Actually the minimum found is slightly higher than the minimum of the exact discretized problem ($-0.460$). This may be due to a double uncertainty and a systematic error. The uncertaintes are one in determining the best values of mu and sigma, and the other in calculating the integral given these values. The systematic error lies in the fact that I arbitrarily chose the space of wave functions on which I applied the variational principle, and this space is much smaller than the natural space where to look for the ground state, i.e., $L^2(\\mathbb{R})$.\n",
    "\n",
    "3) I could therefore improve my results by working on these two uncertainties: for the former I could lower the final temperature, increase the moves for each temperature, and vary the parameters less (try to perform slower cooling); for the latter I could increase the number of blocks and their length in the calculation of the integral. Another way to improve my results is to look for a more expressive form for the wavefunction, which is closer to the true solution of the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
